{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Sentiment Analyis.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JVMqibFVc1ci"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "VgRpPip8dkHW",
    "outputId": "8057e2f9-8682-47ef-c24b-e3fc821060cf"
   },
   "source": [
    "df = pd.read_csv(\"../dataset/sentiment-analysis/Reddit_Data.csv\").rename(columns = {\"clean_comment\":\"clean_text\"}).append(pd.read_csv(\"../dataset/sentiment-analysis/Twitter_Data.csv\"))\n",
    "df.head()"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          clean_text  category\n0   family mormon have never tried explain them t...       1.0\n1  buddhism has very much lot compatible with chr...       1.0\n2  seriously don say thing first all they won get...      -1.0\n3  what you have learned yours and only yours wha...       0.0\n4  for your own benefit you may want read living ...       1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>family mormon have never tried explain them t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>buddhism has very much lot compatible with chr...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>seriously don say thing first all they won get...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what you have learned yours and only yours wha...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>for your own benefit you may want read living ...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHDFzmtpgiS8"
   },
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xtE5T7LvlO62"
   },
   "source": [
    "df = df.astype({'clean_text': str})\n",
    "df.head()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          clean_text  category\n0   family mormon have never tried explain them t...       1.0\n1  buddhism has very much lot compatible with chr...       1.0\n2  seriously don say thing first all they won get...      -1.0\n3  what you have learned yours and only yours wha...       0.0\n4  for your own benefit you may want read living ...       1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>family mormon have never tried explain them t...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>buddhism has very much lot compatible with chr...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>seriously don say thing first all they won get...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what you have learned yours and only yours wha...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>for your own benefit you may want read living ...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys4sxjkcoyPn"
   },
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "clean_text    False\ncategory       True\ndtype: bool"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               clean_text  category\n130448  the foundation stone northeast gas grid inaugu...       NaN\n155642  dear terrorists you can run but you cant hide ...       NaN\n155698  offense the best defence with mission shakti m...       NaN\n155770  have always heard politicians backing out thei...       NaN\n158693  modi government plans felicitate the faceless ...       NaN\n159442               chidambaram gives praises modinomics       NaN\n160559  the reason why modi contested from seats 2014 ...       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>130448</th>\n      <td>the foundation stone northeast gas grid inaugu...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>155642</th>\n      <td>dear terrorists you can run but you cant hide ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>155698</th>\n      <td>offense the best defence with mission shakti m...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>155770</th>\n      <td>have always heard politicians backing out thei...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>158693</th>\n      <td>modi government plans felicitate the faceless ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>159442</th>\n      <td>chidambaram gives praises modinomics</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>160559</th>\n      <td>the reason why modi contested from seats 2014 ...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['category'].isnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Counting word count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "df['token'] = df['clean_text'].apply(nltk.tokenize.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df['len'] = df['token'].apply(len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0          39\n1         196\n2          86\n3          29\n4         112\n         ... \n162975     18\n162976     36\n162977      9\n162978     13\n162979     34\nName: len, Length: 200222, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "count    200222.000000\nmean         21.931296\nstd          26.686952\nmin           0.000000\n25%          10.000000\n50%          18.000000\n75%          30.000000\nmax        1307.000000\nName: len, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['len'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              clean_text  category  \\\n1682    going churlish and ahead and puncture your ba...       1.0   \n6016   the reason very simple copy and paste from red...       1.0   \n29923   india pakistan html for the past few decades ...       1.0   \n24431   modi’ government last four years has recovere...       1.0   \n4184   too much dependence people from iit and iim ca...       1.0   \n10440   demonetisation fails then blame modi told cab...       1.0   \n20979  there redemption for the crimes committed agai...       1.0   \n7231   let one one food beef obviously same thing her...       1.0   \n4373   part you have something mind please let know c...       1.0   \n28490  first all extremely sad people misunderstood y...       1.0   \n34281   read answer quora but direct links aren allow...       1.0   \n23361  hello have few questions related development s...       1.0   \n30639   what your opinion about social media being us...       1.0   \n8646   brilliant point point breakdown one other argu...       1.0   \n\n                                                   token   len  \n1682   [going, churlish, and, ahead, and, puncture, y...  1003  \n6016   [the, reason, very, simple, copy, and, paste, ...  1006  \n29923  [india, pakistan, html, for, the, past, few, d...  1009  \n24431  [modi, ’, government, last, four, years, has, ...  1035  \n4184   [too, much, dependence, people, from, iit, and...  1054  \n10440  [demonetisation, fails, then, blame, modi, tol...  1084  \n20979  [there, redemption, for, the, crimes, committe...  1103  \n7231   [let, one, one, food, beef, obviously, same, t...  1119  \n4373   [part, you, have, something, mind, please, let...  1177  \n28490  [first, all, extremely, sad, people, misunders...  1181  \n34281  [read, answer, quora, but, direct, links, aren...  1207  \n23361  [hello, have, few, questions, related, develop...  1260  \n30639  [what, your, opinion, about, social, media, be...  1303  \n8646   [brilliant, point, point, breakdown, one, othe...  1307  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_text</th>\n      <th>category</th>\n      <th>token</th>\n      <th>len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1682</th>\n      <td>going churlish and ahead and puncture your ba...</td>\n      <td>1.0</td>\n      <td>[going, churlish, and, ahead, and, puncture, y...</td>\n      <td>1003</td>\n    </tr>\n    <tr>\n      <th>6016</th>\n      <td>the reason very simple copy and paste from red...</td>\n      <td>1.0</td>\n      <td>[the, reason, very, simple, copy, and, paste, ...</td>\n      <td>1006</td>\n    </tr>\n    <tr>\n      <th>29923</th>\n      <td>india pakistan html for the past few decades ...</td>\n      <td>1.0</td>\n      <td>[india, pakistan, html, for, the, past, few, d...</td>\n      <td>1009</td>\n    </tr>\n    <tr>\n      <th>24431</th>\n      <td>modi’ government last four years has recovere...</td>\n      <td>1.0</td>\n      <td>[modi, ’, government, last, four, years, has, ...</td>\n      <td>1035</td>\n    </tr>\n    <tr>\n      <th>4184</th>\n      <td>too much dependence people from iit and iim ca...</td>\n      <td>1.0</td>\n      <td>[too, much, dependence, people, from, iit, and...</td>\n      <td>1054</td>\n    </tr>\n    <tr>\n      <th>10440</th>\n      <td>demonetisation fails then blame modi told cab...</td>\n      <td>1.0</td>\n      <td>[demonetisation, fails, then, blame, modi, tol...</td>\n      <td>1084</td>\n    </tr>\n    <tr>\n      <th>20979</th>\n      <td>there redemption for the crimes committed agai...</td>\n      <td>1.0</td>\n      <td>[there, redemption, for, the, crimes, committe...</td>\n      <td>1103</td>\n    </tr>\n    <tr>\n      <th>7231</th>\n      <td>let one one food beef obviously same thing her...</td>\n      <td>1.0</td>\n      <td>[let, one, one, food, beef, obviously, same, t...</td>\n      <td>1119</td>\n    </tr>\n    <tr>\n      <th>4373</th>\n      <td>part you have something mind please let know c...</td>\n      <td>1.0</td>\n      <td>[part, you, have, something, mind, please, let...</td>\n      <td>1177</td>\n    </tr>\n    <tr>\n      <th>28490</th>\n      <td>first all extremely sad people misunderstood y...</td>\n      <td>1.0</td>\n      <td>[first, all, extremely, sad, people, misunders...</td>\n      <td>1181</td>\n    </tr>\n    <tr>\n      <th>34281</th>\n      <td>read answer quora but direct links aren allow...</td>\n      <td>1.0</td>\n      <td>[read, answer, quora, but, direct, links, aren...</td>\n      <td>1207</td>\n    </tr>\n    <tr>\n      <th>23361</th>\n      <td>hello have few questions related development s...</td>\n      <td>1.0</td>\n      <td>[hello, have, few, questions, related, develop...</td>\n      <td>1260</td>\n    </tr>\n    <tr>\n      <th>30639</th>\n      <td>what your opinion about social media being us...</td>\n      <td>1.0</td>\n      <td>[what, your, opinion, about, social, media, be...</td>\n      <td>1303</td>\n    </tr>\n    <tr>\n      <th>8646</th>\n      <td>brilliant point point breakdown one other argu...</td>\n      <td>1.0</td>\n      <td>[brilliant, point, point, breakdown, one, othe...</td>\n      <td>1307</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['len'] > 1000].sort_values('len')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'brilliant point point breakdown one other argument making the rounds false argument okay maybe the strikes took place but should have been kept secret and deniable there good reason for dgmo make statements about covert actions except for bjp score political points there are massive number reasons make strike like this public exposes pakistan bluster just tough talk for all their talk being high alert for indian troop movement along the loc after the uri attack they didn even see coming massively boosts morale within the armed forces morale which has been getting steadily lower our bases kept getting hit while they were not allowed take charge the offensive the troops are also being vilified the media national and international while trying control spiral violence srinagar fueled pakistan propaganda machine makes the best use pakistan blunder overreaching and outmaneuvers them pakistan has its hands tied now after making bold statements about defending against indian adventurism aggression and denying that any terrorist camps even exist the region that they have the support their government pakistan has left itself publicly admissible out can either promise retaliation for strikes thereby confirming the existence such terror bases deny the strike and claim ignorance any bases the area releasing statements and evidence the strikes india makes pakistan look weak and incompetent defense while also making the point that pakistan harbors terrorists and uses them proxy war against india manner that exceeds just providing moral and diplomatic support they have claimed the past calls pakistan bluff threats start nuclear conflict their nuclear arsenal while still being very real and implicit threat longer their impenetrable shield striking within pok and then making public india has taken control the escalatory ladden you said earlier out the open for everyone see which leads the next point makes public the strategic shift indian policy dealing with cross border infiltration and terrorism this now means that india controls the play events and that infiltration will actively fought against taking the fight the terror outposts and leaving open the possibility even striking training camps situated further inland this means that india will make pakistan work and pay for every infiltration attempt literally not just pay terms revenge and bloodshed pakistan will need actually spend more time money resources manpower actual defense and surveillance near these bases their terror outposts will have constantly vigilant and more resources dedicated keeping eye out for incoming strikes the future this very different from the past where the outposts had nothing worry about till they crossed the border and their entire focus was offensive not defensive nature points out very little fencing the pakistani side because they never even felt the need defend themselves but that will all change now terrorists will not sleep peacefully their beds they will possibly make more mistakes and every new person recruited keep watch patrol their perimeter potential vulnerability too accidentally leaked info paid informant mole many these will make increasingly expensive and difficult maintain multiple such bases leading demoralized terrorists demoralized pakistani troops demoralized and disillusioned pakistani public pakistan currently damage control mode firing off more propaganda than artillery and trying pooh pooh the stories the strikes indian propaganda but inaccuracies their accounts will begin add and morale will definitely take hit everyone the pakistani side currently denial but that expected ultimately again makes the government look dishonest incompetent and erodes their imaginary moral high ground that they lay claim strengthens the indian public support for the government this definitely well deserved win for the bjp but more importantly makes the indian public lot more unified behind their central government regardless who was power when such action was taken there are certainly plenty who question the narrative and events but there definite surge public approval favor retaliation for uri and the strikes themselves regardless their position the who how where why covert overt bragging courting war etc this demonstrates among other things that the goi proactive that was receptive the outrage about uri across the nation that was willing take stand such attacks indian sovereignty demonstrates india ability operate within the bounds previous agreements and treaties while still taking decisive action defend itself and not being enslaved those treaties pakistan has always played victim when came any border related issue and deny any involvement previous incursions from their side meanwhile india hamstrung restrictive treaties preventing military retaliation for numerous actions carried out pakistan and its proxy fighters till now this deft navigation not altering the loc while striking terrorist camps falls well within the bounds the simla agreement and the charter surgical strikes across loc line with shimla agreement and charter 3026586 html exploring this avenue publicly means that india can now freely continue perform such strikes the future without invoking any backlash from most other nations and getting active support from several now let assume that the strikes took place but the government kept absolutely secret the outcome not making the strikes public knowledge would have been quite disadvantageous the whole india maintains deniability for aggressive action great but pakistan will constantly play the victim anyway portray cross border terrorism domestic rebellion foment unrest and paint india oppressive bully state committing atrocities kashmir very little advantage here while the strike killed handful terrorists and prevented few attacks the near future would fail adequately capitalize that victory other terror outfits and individual members such organisations may remain unaware and unafraid pakistani turf terrorists sleeping soundly before attempting border crossing isn good for indian defense maintaining state combat ready vigilance and tension impossible for more than few days for all but the most highly trained soldiers demoralizes indian troops they would feel abandoned the govt and the people left deal with bloodthirsty militants all themselves without any chance take action and preempt such attacks the indian soldiers continue the only ones who sleep fear with the fatigue constant vigilance making them weaker while the enemy gets stronger indian silence taken for weakness emboldens our enemies india seen soft target home and abroad india loses control the narrative the few who would aware the covert action the other side could later alter the narrative they have done several times the past and claim india committed atrocities and crimes pakistani territory and killed civilians blaming raw and other agencies and using justify further attacks several years the future the context and events this time would have faded from the public mind and india would the diplomatic back foot having either deny the action ever took place and look guilty forced justify such action and remind the public that was retaliation for uri not random unprovoked attack better claim upfront and connect the two events inextricably forever india retaliates for uri massacre today lot better than india denies committing war crimes pok years later when the conspiracy theorists would have had plenty time concoct alternate narrative pakistan would continue have the facade nuclear shield despite successful strike until challenged head publicly pakistan claims would continue hold weight pakistan retains control the escalatory ladder pakistan bluff not called and continues project the image competent military pakistan retains the facade harboring terrorists its soil what terrorists public acknowledgement shift the policy strategic restraint indian public loses faith the government not knowing about any retaliation would mean that the indian public would just have another item list complain about how the government all talk action critics the government perceived restraint would continue make the government look weak and ineffective bjp would crucified the public eye for not taking stronger measures because the public wouldn know about the strike the voice the people would appear have been ignored future governments would unlikely continue the shift strategy reluctance take control the offensive would persist continued veil secrecy would mean that their efforts would unnoticed while the costs and risks such operations would for little political benefit '"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].iloc[8646]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# df['label'] = df['category'].map({\n",
    "#     -1.0: np.array([1, 0, 0], dtype=float),\n",
    "#     0.0: np.array([0, 1, 0], dtype=float),\n",
    "#     1.0: np.array([0, 0, 1], dtype=float)\n",
    "# })\n",
    "# df.drop('label', axis=1)\n",
    "df['label'] = df['category'].map({\n",
    "    -1.0: [1, 0, 0],\n",
    "    0.0: [0, 1, 0],\n",
    "    1.0: [0, 0, 1]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          clean_text  category  \\\n0   family mormon have never tried explain them t...       1.0   \n1  buddhism has very much lot compatible with chr...       1.0   \n2  seriously don say thing first all they won get...      -1.0   \n3  what you have learned yours and only yours wha...       0.0   \n4  for your own benefit you may want read living ...       1.0   \n\n                                               token  len      label  \n0  [family, mormon, have, never, tried, explain, ...   39  [0, 0, 1]  \n1  [buddhism, has, very, much, lot, compatible, w...  196  [0, 0, 1]  \n2  [seriously, don, say, thing, first, all, they,...   86  [1, 0, 0]  \n3  [what, you, have, learned, yours, and, only, y...   29  [0, 1, 0]  \n4  [for, your, own, benefit, you, may, want, read...  112  [0, 0, 1]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_text</th>\n      <th>category</th>\n      <th>token</th>\n      <th>len</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>family mormon have never tried explain them t...</td>\n      <td>1.0</td>\n      <td>[family, mormon, have, never, tried, explain, ...</td>\n      <td>39</td>\n      <td>[0, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>buddhism has very much lot compatible with chr...</td>\n      <td>1.0</td>\n      <td>[buddhism, has, very, much, lot, compatible, w...</td>\n      <td>196</td>\n      <td>[0, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>seriously don say thing first all they won get...</td>\n      <td>-1.0</td>\n      <td>[seriously, don, say, thing, first, all, they,...</td>\n      <td>86</td>\n      <td>[1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what you have learned yours and only yours wha...</td>\n      <td>0.0</td>\n      <td>[what, you, have, learned, yours, and, only, y...</td>\n      <td>29</td>\n      <td>[0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>for your own benefit you may want read living ...</td>\n      <td>1.0</td>\n      <td>[for, your, own, benefit, you, may, want, read...</td>\n      <td>112</td>\n      <td>[0, 0, 1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vLAIMiIEoxL3"
   },
   "source": [
    "sample = df.sample(frac=1.0, random_state=4)\n",
    "train = sample[:int(0.8*len(sample))]\n",
    "test = sample[int(0.8*len(sample)):]"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B3rqexT-gX0v"
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['clean_text'].values)\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WUqF-M8TlCkK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20b5ca6c-f8ca-490a-9762-c2985cbeea7c"
   },
   "source": [
    "print(len(tokenizer.word_index))\n",
    "len(tokenizer.word_index)**0.25\n"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121047\n"
     ]
    },
    {
     "data": {
      "text/plain": "18.65256266064929"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WcTVRSXYmKdw"
   },
   "source": [
    "# a = \"I remember it all too well\"\n",
    "# tokenizer.texts_to_sequences([a])"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PV3M1LfNmz9p"
   },
   "source": [
    "sequence_train = tokenizer.texts_to_sequences(train[\"clean_text\"])\n",
    "sequence_test = tokenizer.texts_to_sequences(test[\"clean_text\"])"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D_bl8yNAnOLm"
   },
   "source": [
    "pad_train= tf.keras.preprocessing.sequence.pad_sequences(sequence_train, maxlen=50, padding='pre')\n",
    "pad_test= tf.keras.preprocessing.sequence.pad_sequences(sequence_test, maxlen=50, padding='pre')\n"
   ],
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hPB61QUw2MN"
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbDcaa6Bw1-D",
    "outputId": "83cb0035-f143-4c9e-d6e6-8616540ac076"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#hidden and output layer\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "     tf.keras.layers.Embedding(len(tokenizer.word_index)+1,64,input_length=50, trainable=True), #total vocab, output (dari total vocab akar 4)\n",
    "     layers.LSTM(64, name=\"hai_shaq\"), #hidden layer\n",
    "     # layers.LSTM(64, return_sequences=True), #hidden layer\n",
    "     # layers.LSTM(64, return_sequences=True), #hidden layer\n",
    "     # layers.LSTM(64, return_sequences=True), #hidden layer\n",
    "     # layers.LSTM(64), #hidden layer\n",
    "     # layers.Dense(64, activation=tf.nn.relu),\n",
    "     layers.Dense(64, activation=tf.nn.relu),\n",
    "     # layers.Dense(3, activation =\"softmax\", name=\"layer3\") #output layer\n",
    "     layers.Dense(3, activation =\"softmax\", name=\"layer3\") #output layer\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ],
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 50, 64)            7747072   \n",
      "                                                                 \n",
      " hai_shaq (LSTM)             (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,784,451\n",
      "Trainable params: 7,784,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA4_EbaV3bd4"
   },
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nSM_ZaDxqdUo"
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"categorical_crossentropy\" , metrics=['accuracy'])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"categorical_crossentropy\" , metrics=['accuracy'])"
   ],
   "execution_count": 93,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTFSXU8g4oe6"
   },
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0], shape=(3,), dtype=int32)\n",
      "[1, 0, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3108/1770709347.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'label'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'label'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    104\u001B[0m       \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m   \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 106\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEagerTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "print(tf.convert_to_tensor([1, 0, 0]))\n",
    "print(train['label'].iloc[0])\n",
    "print(tf.convert_to_tensor(train['label'].values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "20126    -1.0\n160018    1.0\n13740     0.0\n118913    0.0\n16820     1.0\n         ... \n20433     0.0\n64887     0.0\n59823     1.0\n17789    -1.0\n92015     1.0\nName: category, Length: 160177, dtype: float64"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['category']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "        -1.0   0.0   1.0\n20126      1     0     0\n160018     0     0     1\n13740      0     1     0\n118913     0     1     0\n16820      0     0     1\n...      ...   ...   ...\n20433      0     1     0\n64887      0     1     0\n59823      0     0     1\n17789      1     0     0\n92015      0     0     1\n\n[160177 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-1.0</th>\n      <th>0.0</th>\n      <th>1.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20126</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>160018</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13740</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>118913</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16820</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20433</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64887</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59823</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17789</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>92015</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>160177 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(train['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fq68mIbC4oB1",
    "outputId": "412e442d-6fb2-4d8a-f9d0-5763b63d9c2a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "history = model.fit(x=pad_train, y=pd.get_dummies(train['category']), batch_size=32, epochs=100, verbose=1,\n",
    "    validation_data=[pad_test,pd.get_dummies(test['category'])],\n",
    "          steps_per_epoch=32)"
   ],
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 9s 237ms/step - loss: 1.0461 - accuracy: 0.4346 - val_loss: 1.0028 - val_accuracy: 0.5289\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.9726 - accuracy: 0.5391 - val_loss: 0.9637 - val_accuracy: 0.5471\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.9612 - accuracy: 0.5449 - val_loss: 0.9343 - val_accuracy: 0.5616\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.9402 - accuracy: 0.5557 - val_loss: 0.8829 - val_accuracy: 0.5969\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.8611 - accuracy: 0.6201 - val_loss: 0.8312 - val_accuracy: 0.6217\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 0.8059 - accuracy: 0.6309 - val_loss: 0.7561 - val_accuracy: 0.6592\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 0.7508 - accuracy: 0.6582 - val_loss: 0.7354 - val_accuracy: 0.6909\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.7060 - accuracy: 0.7041 - val_loss: 0.6682 - val_accuracy: 0.7185\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.6506 - accuracy: 0.7168 - val_loss: 0.5925 - val_accuracy: 0.7589\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.5829 - accuracy: 0.7812 - val_loss: 0.5685 - val_accuracy: 0.7718\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.5241 - accuracy: 0.7930 - val_loss: 0.5542 - val_accuracy: 0.7912\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.5216 - accuracy: 0.8086 - val_loss: 0.5337 - val_accuracy: 0.8046\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.4848 - accuracy: 0.8213 - val_loss: 0.4826 - val_accuracy: 0.8177\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.4998 - accuracy: 0.8125 - val_loss: 0.4373 - val_accuracy: 0.8396\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.4037 - accuracy: 0.8447 - val_loss: 0.4194 - val_accuracy: 0.8469\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.4502 - accuracy: 0.8418 - val_loss: 0.4747 - val_accuracy: 0.8520\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.4231 - accuracy: 0.8535 - val_loss: 0.3884 - val_accuracy: 0.8619\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.3976 - accuracy: 0.8643 - val_loss: 0.3945 - val_accuracy: 0.8648\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.3607 - accuracy: 0.8809 - val_loss: 0.3655 - val_accuracy: 0.8752\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.3762 - accuracy: 0.8779 - val_loss: 0.3507 - val_accuracy: 0.8822\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.3286 - accuracy: 0.8896 - val_loss: 0.3450 - val_accuracy: 0.8848\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 7s 213ms/step - loss: 0.2903 - accuracy: 0.9062 - val_loss: 0.3318 - val_accuracy: 0.8891\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.2902 - accuracy: 0.9053 - val_loss: 0.3360 - val_accuracy: 0.8855\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 0.2973 - accuracy: 0.9111 - val_loss: 0.3409 - val_accuracy: 0.8825\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.3175 - accuracy: 0.8848 - val_loss: 0.3220 - val_accuracy: 0.8943\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.3027 - accuracy: 0.8965 - val_loss: 0.3074 - val_accuracy: 0.8995\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2828 - accuracy: 0.9111 - val_loss: 0.2983 - val_accuracy: 0.9017\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2782 - accuracy: 0.9150 - val_loss: 0.3161 - val_accuracy: 0.8948\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.3046 - accuracy: 0.9072 - val_loss: 0.2985 - val_accuracy: 0.9052\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.2802 - accuracy: 0.9053 - val_loss: 0.2838 - val_accuracy: 0.9081\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.2871 - accuracy: 0.9092 - val_loss: 0.2697 - val_accuracy: 0.9144\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2789 - accuracy: 0.9150 - val_loss: 0.2791 - val_accuracy: 0.9113\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.2668 - accuracy: 0.9131 - val_loss: 0.2657 - val_accuracy: 0.9171\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2553 - accuracy: 0.9170 - val_loss: 0.2580 - val_accuracy: 0.9191\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2707 - accuracy: 0.9141 - val_loss: 0.2702 - val_accuracy: 0.9162\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.2726 - accuracy: 0.9121 - val_loss: 0.2660 - val_accuracy: 0.9135\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.2622 - accuracy: 0.9189 - val_loss: 0.2487 - val_accuracy: 0.9215\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2614 - accuracy: 0.9238 - val_loss: 0.2451 - val_accuracy: 0.9245\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.1895 - accuracy: 0.9463 - val_loss: 0.2500 - val_accuracy: 0.9217\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2366 - accuracy: 0.9248 - val_loss: 0.2446 - val_accuracy: 0.9229\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 7s 211ms/step - loss: 0.2693 - accuracy: 0.9092 - val_loss: 0.2478 - val_accuracy: 0.9223\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 6s 205ms/step - loss: 0.2553 - accuracy: 0.9229 - val_loss: 0.2387 - val_accuracy: 0.9267\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 6s 203ms/step - loss: 0.2218 - accuracy: 0.9297 - val_loss: 0.2324 - val_accuracy: 0.9278\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 7s 213ms/step - loss: 0.2216 - accuracy: 0.9355 - val_loss: 0.2284 - val_accuracy: 0.9308\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 7s 209ms/step - loss: 0.2206 - accuracy: 0.9385 - val_loss: 0.2333 - val_accuracy: 0.9271\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 7s 213ms/step - loss: 0.2132 - accuracy: 0.9287 - val_loss: 0.2299 - val_accuracy: 0.9311\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2162 - accuracy: 0.9287 - val_loss: 0.2247 - val_accuracy: 0.9294\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.2365 - accuracy: 0.9268 - val_loss: 0.2191 - val_accuracy: 0.9326\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 7s 212ms/step - loss: 0.1971 - accuracy: 0.9453 - val_loss: 0.2271 - val_accuracy: 0.9291\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2071 - accuracy: 0.9316 - val_loss: 0.2209 - val_accuracy: 0.9319\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.2045 - accuracy: 0.9414 - val_loss: 0.2135 - val_accuracy: 0.9355\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1694 - accuracy: 0.9482 - val_loss: 0.2154 - val_accuracy: 0.9347\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1909 - accuracy: 0.9404 - val_loss: 0.2278 - val_accuracy: 0.9333\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.2480 - accuracy: 0.9238 - val_loss: 0.2136 - val_accuracy: 0.9354\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.2168 - accuracy: 0.9365 - val_loss: 0.2135 - val_accuracy: 0.9352\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 0.2167 - accuracy: 0.9355 - val_loss: 0.2156 - val_accuracy: 0.9359\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 0.2258 - accuracy: 0.9258 - val_loss: 0.2163 - val_accuracy: 0.9344\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 7s 213ms/step - loss: 0.2013 - accuracy: 0.9443 - val_loss: 0.2041 - val_accuracy: 0.9393\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.1980 - accuracy: 0.9355 - val_loss: 0.2013 - val_accuracy: 0.9394\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 7s 211ms/step - loss: 0.2227 - accuracy: 0.9336 - val_loss: 0.2043 - val_accuracy: 0.9398\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 6s 207ms/step - loss: 0.2195 - accuracy: 0.9355 - val_loss: 0.2033 - val_accuracy: 0.9387\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 7s 210ms/step - loss: 0.2207 - accuracy: 0.9336 - val_loss: 0.1997 - val_accuracy: 0.9409\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 7s 211ms/step - loss: 0.1988 - accuracy: 0.9404 - val_loss: 0.1965 - val_accuracy: 0.9412\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 6s 207ms/step - loss: 0.1473 - accuracy: 0.9561 - val_loss: 0.1985 - val_accuracy: 0.9409\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 6s 205ms/step - loss: 0.1639 - accuracy: 0.9531 - val_loss: 0.1982 - val_accuracy: 0.9415\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 6s 207ms/step - loss: 0.2122 - accuracy: 0.9375 - val_loss: 0.2035 - val_accuracy: 0.9399\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 6s 207ms/step - loss: 0.2398 - accuracy: 0.9365 - val_loss: 0.1957 - val_accuracy: 0.9431\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 6s 206ms/step - loss: 0.1776 - accuracy: 0.9541 - val_loss: 0.1935 - val_accuracy: 0.9422\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 6s 208ms/step - loss: 0.2099 - accuracy: 0.9453 - val_loss: 0.1895 - val_accuracy: 0.9423\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 6s 205ms/step - loss: 0.1839 - accuracy: 0.9443 - val_loss: 0.1888 - val_accuracy: 0.9422\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 7s 212ms/step - loss: 0.1740 - accuracy: 0.9443 - val_loss: 0.1946 - val_accuracy: 0.9395\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 6s 202ms/step - loss: 0.1862 - accuracy: 0.9414 - val_loss: 0.1908 - val_accuracy: 0.9412\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 6s 205ms/step - loss: 0.1456 - accuracy: 0.9580 - val_loss: 0.1889 - val_accuracy: 0.9418\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 7s 209ms/step - loss: 0.1787 - accuracy: 0.9473 - val_loss: 0.1885 - val_accuracy: 0.9429\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 6s 208ms/step - loss: 0.1806 - accuracy: 0.9434 - val_loss: 0.1878 - val_accuracy: 0.9449\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 6s 203ms/step - loss: 0.1721 - accuracy: 0.9414 - val_loss: 0.1921 - val_accuracy: 0.9399\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 6s 207ms/step - loss: 0.1529 - accuracy: 0.9541 - val_loss: 0.1793 - val_accuracy: 0.9463\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 7s 214ms/step - loss: 0.1771 - accuracy: 0.9473 - val_loss: 0.1925 - val_accuracy: 0.9410\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 7s 216ms/step - loss: 0.1841 - accuracy: 0.9443 - val_loss: 0.1837 - val_accuracy: 0.9449\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.2262 - accuracy: 0.9238 - val_loss: 0.1846 - val_accuracy: 0.9456\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 7s 216ms/step - loss: 0.1598 - accuracy: 0.9541 - val_loss: 0.1818 - val_accuracy: 0.9455\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.1685 - accuracy: 0.9453 - val_loss: 0.1808 - val_accuracy: 0.9452\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.2186 - accuracy: 0.9395 - val_loss: 0.1814 - val_accuracy: 0.9465\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1700 - accuracy: 0.9473 - val_loss: 0.1798 - val_accuracy: 0.9482\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.1682 - accuracy: 0.9502 - val_loss: 0.1779 - val_accuracy: 0.9461\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.1493 - accuracy: 0.9551 - val_loss: 0.1728 - val_accuracy: 0.9501\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.1697 - accuracy: 0.9492 - val_loss: 0.1866 - val_accuracy: 0.9438\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 0.1829 - accuracy: 0.9443 - val_loss: 0.1770 - val_accuracy: 0.9469\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.1413 - accuracy: 0.9551 - val_loss: 0.1761 - val_accuracy: 0.9491\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1642 - accuracy: 0.9531 - val_loss: 0.1749 - val_accuracy: 0.9488\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.1640 - accuracy: 0.9463 - val_loss: 0.1729 - val_accuracy: 0.9481\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1718 - accuracy: 0.9512 - val_loss: 0.1701 - val_accuracy: 0.9493\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.1731 - accuracy: 0.9482 - val_loss: 0.1747 - val_accuracy: 0.9468\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 0.1411 - accuracy: 0.9600 - val_loss: 0.1789 - val_accuracy: 0.9461\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 6s 208ms/step - loss: 0.1542 - accuracy: 0.9502 - val_loss: 0.1716 - val_accuracy: 0.9464\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 6s 206ms/step - loss: 0.1690 - accuracy: 0.9443 - val_loss: 0.1649 - val_accuracy: 0.9504\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 6s 205ms/step - loss: 0.1998 - accuracy: 0.9326 - val_loss: 0.1782 - val_accuracy: 0.9497\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 6s 201ms/step - loss: 0.1795 - accuracy: 0.9463 - val_loss: 0.1709 - val_accuracy: 0.9471\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 6s 206ms/step - loss: 0.1546 - accuracy: 0.9492 - val_loss: 0.1623 - val_accuracy: 0.9510\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 7s 216ms/step - loss: 0.1737 - accuracy: 0.9424 - val_loss: 0.1726 - val_accuracy: 0.9461\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_29_layer_call_fn, lstm_cell_29_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn, lstm_cell_29_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/sentiment-model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/sentiment-model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002138364C460> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('../model/sentiment-model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "prediction = model.predict(pad_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction.round())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.get_dummies(test['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9399925084280185"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(pd.get_dummies(test['category']), prediction.round())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3108/441085979.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'accuracy'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "history.history['accuracy']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(len(history.history['accuracy'])), history.history['accuracy'], 'r')\n",
    "plt.plot(range(len(history.history['accuracy'])), history.history['val_accuracy'], 'y')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}