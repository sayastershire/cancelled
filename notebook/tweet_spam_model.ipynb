{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "DF_LOC = '../dataset/twitter-spam/train.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Base Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "raw_df = pd.read_csv(DF_LOC).drop('following', axis=1).drop('followers', axis=1).drop('actions', axis=1).drop('is_retweet', axis=1).drop('location', axis=1).drop('Unnamed: 7', axis=1)\n",
    "raw_df.iloc[7552]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Tweet    #Virgos are usually very helpful around the ho...\n",
       "Type                                          South Dakota\n",
       "Name: 7552, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Preprocess type\n",
    "raw_df['Type'] = raw_df['Type'].map({'Quality': 0, 'Spam': 1})\n",
    "raw_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Morning Love  @LeeBrown_V</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'@realDonaldTrump @USNavy RIP TO HEROES'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haven't been following the news but I understa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pic.twitter.com/dy9q4ftLhZ What to do with pap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#DidYouKnow ► Mahatma Gandhi made a brief visi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Type\n",
       "0                     Good Morning Love  @LeeBrown_V   0.0\n",
       "1           '@realDonaldTrump @USNavy RIP TO HEROES'   1.0\n",
       "2  Haven't been following the news but I understa...   0.0\n",
       "3  pic.twitter.com/dy9q4ftLhZ What to do with pap...   0.0\n",
       "4  #DidYouKnow ► Mahatma Gandhi made a brief visi...   0.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "raw_df = raw_df.drop(raw_df['Type'].loc[raw_df['Type'].isnull()].index.values)\n",
    "# raw_df.isnull()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "raw_df['Type'].loc[raw_df['Type'].isnull()]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Series([], Name: Type, dtype: float64)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sentences = raw_df['Tweet']\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sentences"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                           Good Morning Love  @LeeBrown_V\n",
       "1                 '@realDonaldTrump @USNavy RIP TO HEROES'\n",
       "2        Haven't been following the news but I understa...\n",
       "3        pic.twitter.com/dy9q4ftLhZ What to do with pap...\n",
       "4        #DidYouKnow ► Mahatma Gandhi made a brief visi...\n",
       "                               ...                        \n",
       "14894    #AllWentWrongWhen I told my hair stylist to \"g...\n",
       "14895    They don't have to like you, and you don't hav...\n",
       "14896    #Miami Graham Nash Live at Parker Playhouse  #...\n",
       "14897    @bethannhamilton is in the business of one-upp...\n",
       "14898      Chasing Success  by  Space Cadetz  Listen up...\n",
       "Name: Tweet, Length: 14897, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Saving via pickle\n",
    "import pickle\n",
    "with open('../dataset/basic-tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   86   233    73 11601   158     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0]\n",
      "(14897, 71)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# len(tokenizer.word_index)\n",
    "padded[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([   86,   233,    73, 11601,   158,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index), 16, input_length=71),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 71, 16)            649152    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 670,145\n",
      "Trainable params: 670,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "model.fit(padded, raw_df['Type'].values, epochs=150)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "466/466 [==============================] - 15s 26ms/step - loss: 0.2957 - binary_accuracy: 0.8642\n",
      "Epoch 2/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0995 - binary_accuracy: 0.9635\n",
      "Epoch 3/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0405 - binary_accuracy: 0.9881\n",
      "Epoch 4/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0185 - binary_accuracy: 0.9944\n",
      "Epoch 5/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0081 - binary_accuracy: 0.9974\n",
      "Epoch 6/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0067 - binary_accuracy: 0.9987\n",
      "Epoch 7/150\n",
      "466/466 [==============================] - 10s 22ms/step - loss: 0.0039 - binary_accuracy: 0.9987\n",
      "Epoch 8/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0061 - binary_accuracy: 0.9979\n",
      "Epoch 9/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0024 - binary_accuracy: 0.9994\n",
      "Epoch 10/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0035 - binary_accuracy: 0.9991\n",
      "Epoch 11/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0019 - binary_accuracy: 0.9993\n",
      "Epoch 12/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.5540e-04 - binary_accuracy: 0.9998\n",
      "Epoch 13/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.6934e-05 - binary_accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.3970e-04 - binary_accuracy: 0.9999\n",
      "Epoch 15/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.5748e-05 - binary_accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0097 - binary_accuracy: 0.9968\n",
      "Epoch 17/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0039 - binary_accuracy: 0.9987\n",
      "Epoch 18/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.5109e-04 - binary_accuracy: 0.9998\n",
      "Epoch 19/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.4318e-05 - binary_accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.1859e-05 - binary_accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.6328e-05 - binary_accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 9.2488e-06 - binary_accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.0495e-06 - binary_accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.0301e-06 - binary_accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.0591e-06 - binary_accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.3207e-06 - binary_accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.4191e-06 - binary_accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.7831e-06 - binary_accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4390e-06 - binary_accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0981e-06 - binary_accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.5899e-07 - binary_accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.1461e-07 - binary_accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.5902e-07 - binary_accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.9751e-07 - binary_accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.7144e-07 - binary_accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8279e-07 - binary_accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.2331e-07 - binary_accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 9.5313e-08 - binary_accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 7.4430e-08 - binary_accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.8645e-08 - binary_accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.2886e-08 - binary_accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.2346e-08 - binary_accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.4534e-08 - binary_accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.9172e-08 - binary_accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.9846e-08 - binary_accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4261e-08 - binary_accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0239e-08 - binary_accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.9722e-09 - binary_accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 6.2017e-09 - binary_accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.1287e-09 - binary_accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "466/466 [==============================] - 10s 22ms/step - loss: 4.6383e-09 - binary_accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.4907e-09 - binary_accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.8069e-09 - binary_accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 2.3852e-09 - binary_accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8170e-09 - binary_accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4204e-09 - binary_accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.3068e-09 - binary_accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0913e-09 - binary_accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.2841e-09 - binary_accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.0042e-10 - binary_accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.2797e-10 - binary_accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.6177e-10 - binary_accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.0664e-10 - binary_accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.2421e-10 - binary_accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.0428e-10 - binary_accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.8245e-10 - binary_accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.1515e-10 - binary_accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.0563e-10 - binary_accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.5007e-10 - binary_accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 2.3260e-10 - binary_accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.3736e-10 - binary_accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.2638e-10 - binary_accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.2073e-10 - binary_accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.9528e-10 - binary_accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8948e-10 - binary_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8841e-10 - binary_accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 1.6356e-10 - binary_accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8592e-10 - binary_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.6964e-10 - binary_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4503e-10 - binary_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.5206e-10 - binary_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.8730e-10 - binary_accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "466/466 [==============================] - 10s 22ms/step - loss: 0.0154 - binary_accuracy: 0.9958\n",
      "Epoch 84/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0060 - binary_accuracy: 0.9987\n",
      "Epoch 85/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.8501e-04 - binary_accuracy: 0.9999\n",
      "Epoch 86/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 3.4167e-05 - binary_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.8298e-05 - binary_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.1883e-05 - binary_accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.3311e-06 - binary_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.9436e-06 - binary_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.2818e-06 - binary_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.0916e-06 - binary_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.2567e-06 - binary_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.6696e-06 - binary_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.2269e-06 - binary_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 9.0643e-07 - binary_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.6941e-07 - binary_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.7074e-07 - binary_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.2103e-07 - binary_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.1031e-07 - binary_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.3743e-07 - binary_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 9.3931e-08 - binary_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.6405e-08 - binary_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.8139e-08 - binary_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.7015e-08 - binary_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.8293e-08 - binary_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.2220e-08 - binary_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.7418e-08 - binary_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.3698e-08 - binary_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.1268e-08 - binary_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.9966e-09 - binary_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 7.2456e-09 - binary_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.9256e-09 - binary_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.7605e-09 - binary_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.9460e-09 - binary_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.1396e-09 - binary_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.5825e-09 - binary_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.1087e-09 - binary_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.7567e-09 - binary_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4650e-09 - binary_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.1894e-09 - binary_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0074e-09 - binary_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 8.4686e-10 - binary_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 7.0487e-10 - binary_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 6.0043e-10 - binary_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 5.0890e-10 - binary_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 4.3568e-10 - binary_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.8024e-10 - binary_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 3.3238e-10 - binary_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.9142e-10 - binary_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.5971e-10 - binary_accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 2.3206e-10 - binary_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 2.0945e-10 - binary_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.9243e-10 - binary_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.7952e-10 - binary_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.7024e-10 - binary_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.5730e-10 - binary_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4933e-10 - binary_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.4045e-10 - binary_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.3485e-10 - binary_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.2495e-10 - binary_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "466/466 [==============================] - 10s 23ms/step - loss: 1.2140e-10 - binary_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.2284e-10 - binary_accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.1583e-10 - binary_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "466/466 [==============================] - 12s 25ms/step - loss: 1.1666e-10 - binary_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0663e-10 - binary_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0646e-10 - binary_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 1.0265e-10 - binary_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0651e-10 - binary_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 1.0341e-10 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8942661070>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "model.save('../model/twitter-spam')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-27 19:32:32.794185: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/twitter-spam/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/twitter-spam/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Testing\n",
    "squid_df = pd.read_json('../dataset/squid-tweets/squid_tweets.json')\n",
    "squid_df = squid_df.rename(columns={0: \"tweets\"})\n",
    "squid_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @sweetshrubs_: More skts x squid game doodl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milesSI Squid Game any good?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just watched episode S01 | E02 of Squid G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @binseolovely: SQUID GAME (#STRAYKIDS VER.)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @a_leesha1: We kinda already have a real li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  RT @sweetshrubs_: More skts x squid game doodl...\n",
       "1                      @milesSI Squid Game any good?\n",
       "2  I've just watched episode S01 | E02 of Squid G...\n",
       "3  RT @binseolovely: SQUID GAME (#STRAYKIDS VER.)...\n",
       "4  RT @a_leesha1: We kinda already have a real li..."
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "input_pred = tokenizer.texts_to_sequences(squid_df['tweets'])\n",
    "# print(input_pred, end='\\n===\\n')\n",
    "input_pad = tf.keras.preprocessing.sequence.pad_sequences(input_pred, padding='post', maxlen=71)\n",
    "# print(input_pad, end='\\n===\\n')\n",
    "is_spam = model.predict(input_pad)\n",
    "\n",
    "squid_df['is_spam'] = [True if i >= 0.5 else False for i in is_spam]\n",
    "\n",
    "# for i in squid_df['tweets']:\n",
    "#     input_pred = tokenizer.texts_to_sequences(i)\n",
    "#     print(input_pred, end='\\n===\\n')\n",
    "#     input_pad = tf.keras.preprocessing.sequence.pad_sequences([input_pred], padding='post', maxlen=71)\n",
    "#     print(input_pad, end='\\n===\\n')\n",
    "#     model.predict(input_pad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "squid_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @sweetshrubs_: More skts x squid game doodl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milesSI Squid Game any good?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just watched episode S01 | E02 of Squid G...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @binseolovely: SQUID GAME (#STRAYKIDS VER.)...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @a_leesha1: We kinda already have a real li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  is_spam\n",
       "0  RT @sweetshrubs_: More skts x squid game doodl...     True\n",
       "1                      @milesSI Squid Game any good?    False\n",
       "2  I've just watched episode S01 | E02 of Squid G...     True\n",
       "3  RT @binseolovely: SQUID GAME (#STRAYKIDS VER.)...     True\n",
       "4  RT @a_leesha1: We kinda already have a real li...    False"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "!pip install openpyxl\n",
    "squid_df.to_excel('../dataset/squid-tweets/squid-dataset.xlsx')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 527 kB/s \n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "squid_df['spam'] = model.predict()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}